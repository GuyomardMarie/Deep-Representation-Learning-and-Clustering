{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Functions provided on our Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run Word2Vec_Functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data must be a codes sequence per patient: \n",
    "\n",
    "codes_par_patient = [[Code1, Code2, Code3], [Patient 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_par_patient = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a dictionnary of unique medical codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format {'code' : id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch of hyperparamaters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested hyperparameters\n",
    "- window_size: number of neighboors \n",
    "- embedding_size: dimension of the codes latent space\n",
    "- batch_size: size of batchs\n",
    "- num_negative_samples: number of randomly false neighboors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size_list = [5, 10]\n",
    "num_negative_samples_list = [5, 10]\n",
    "batch_size_list = [250]\n",
    "embedding_dim_list = [10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hyperparameters = Gridsearch_Train_Word2Vec(codes_par_patient, vocab, epochs, lr, window_size_list, num_negative_samples_list, batch_size_list, embedding_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal hyperparameters\n",
    "\n",
    "We keep the set of hyperparameters minimizing the validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimal window size :' + str(df_hyperparameters.iloc[np.argmin(df_hyperparameters['Total_Loss'])].window_size))\n",
    "print('Optimal number of negative samples :' + str(df_hyperparameters.iloc[np.argmin(df_hyperparameters['Total_Loss'])].num_negative_samples))\n",
    "print('Optimal dimension of the latent space :' + str(df_hyperparameters.iloc[np.argmin(df_hyperparameters['Total_Loss'])].embedding_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the Codes Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = df_hyperparameters.iloc[np.argmin(df_hyperparameters['Total_Loss'])].window_size\n",
    "num_negative_samples = df_hyperparameters.iloc[np.argmin(df_hyperparameters['Total_Loss'])].num_negative_samples\n",
    "embedding_dim = df_hyperparameters.iloc[np.argmin(df_hyperparameters['Total_Loss'])].embedding_dim\n",
    "batch_size = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = Word2VecDataset(codes_par_patient, vocab, window_size, num_negative_samples=num_negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split of the dataset into a train and a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs_train, word_pairs_test= train_test_split(word_pairs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(word_pairs_train, batch_size=250, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Step\n",
    "\n",
    "Param√®tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 200\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Train_Word2Vec(dataloader, vocab, embedding_dim, nb_epoch, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train, dataloader_test = DataLoader(word_pairs_train, batch_size=len(word_pairs_train), shuffle=True), DataLoader(word_pairs_test, batch_size=len(word_pairs_test), shuffle=True)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for target, context, label in dataloader_train:\n",
    "    target, context, label = target.to(device), context.to(device), label.to(device)\n",
    "    output = model(target, context)\n",
    "    label_tensor = label.clone().detach().view(label.size()[0], 1).to(dtype=torch.float)\n",
    "    loss_train = nn.BCEWithLogitsLoss()(output, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for target, context, label in dataloader_test:\n",
    "    target, context, label = target.to(device), context.to(device), label.to(device)\n",
    "    output = model(target, context)\n",
    "    label_tensor = label.clone().detach().view(label.size()[0], 1).to(dtype=torch.float)\n",
    "    loss_test = nn.BCEWithLogitsLoss()(output, label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loss on train : ' + str(round(np.array(loss_train.cpu().detach()).item(),4)))\n",
    "print('Loss on test : ' + str(round(np.array(loss_test.cpu().detach()).item(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Codes Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_representations = np.array(model.cpu().state_dict()['embeddings.weight'].detach().data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('codes_representations', code_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_representation = []\n",
    "for sentence in codes_par_patient :\n",
    "    sentence_embeds = torch.zeros(embedding_dim)\n",
    "    for code in sentence:\n",
    "        word_embed = model.cpu().embeddings(torch.tensor(vocab[code]))\n",
    "        sentence_embeds += word_embed\n",
    "    patient_representation.append(sentence_embeds.detach().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('patients_embeddings', patient_representation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Calcul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
