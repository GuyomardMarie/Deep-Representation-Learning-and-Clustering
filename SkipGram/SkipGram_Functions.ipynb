{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram\n",
    "\n",
    "This notebook provides function to compute Skip-Gram algorithm in order to learn patient representation.\n",
    "\n",
    "### References \n",
    "\n",
    "Distributed Representations of Words and Phrases and their Compositionality, Tomas Mikolov et al (2013)\n",
    "\n",
    "\n",
    "Learning Low-Dimensional Representations of Medical Concepts, Youngduck Choi (2016)\n",
    "\n",
    "\n",
    "Medical Concept Representation Learning from Electronic Health Records and its Application on Heart Failure Prediction, Edward Choi (2016)\n",
    "\n",
    "\n",
    "Patient Embeddings From Diagnosis Codes for Health Care Prediction Tasks: Pat2Vec Machine Learning Framework, Steiger (2023)\n",
    "\n",
    "\n",
    "Doctor AI: Predicting Clinical Events via Recurrent Neural Networks, Edward Choi (2016)\n",
    "\n",
    "\n",
    "#### Python environment \n",
    "- Python 3.10\n",
    "- Required Packages: pytorch, pandas and scikit-learn\n",
    "\n",
    "\n",
    "#### Functionalities\n",
    "\n",
    "- A function to prepare the data: Word2VecDataset\n",
    "- A class for the model: Word2Vec\n",
    "- A Gridsearch of the hyperparameters function: Gridsearch_Train_Word2Vec\n",
    "- A training function: Train_Word2Vec\n",
    "\n",
    "For all these functionalities, explanations are detailed in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates word pairs.\n",
    "\n",
    "For each time sequence (i.e., each patient):\n",
    "\n",
    "- It iterates over each code to find its neighbors. If window_size = 5, then it takes 5 neighbors, ie. the 5 codes following the target one.\n",
    "- It generates 'negative samples': num_negative_samples indicates the number of negative samples.\n",
    "\n",
    "\n",
    "This function returns a list of tuples (target_word, context_word, label).\n",
    "\n",
    "The label indicates if it is a positive sample (a true neighbor) or a negative one (a false one).\n",
    "\n",
    "Example:\n",
    "If window_size = 5 and num_negative_samples = 3, then one obtains:\n",
    "\n",
    "[[451, 525, 1],\n",
    "\n",
    " [451, 817, 1],\n",
    "\n",
    " [451, 818, 1],\n",
    "\n",
    " [451, 579, 1],\n",
    "\n",
    " [451, 579, 1],\n",
    "\n",
    " [451, 530, 0],\n",
    "\n",
    " [451, 530, 0],\n",
    " \n",
    " [451, 659, 0],...]\n",
    "\n",
    "with 525, 817, 818, 579 the true neighbors (label=1) of the target code 451 and 530, 659 the false ones (label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Word2VecDataset(corpus, vocab, window_size, num_negative_samples=0) :\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list\n",
    "        Codes sequence per patient : [[Code1, Code2, Code3], [Patient 2]]\n",
    "    vocab : dict\n",
    "        {'mot' : id}\n",
    "    window_size : int\n",
    "        Size of the window (ie. number of neighboors)\n",
    "    num_negative_samples : int\n",
    "        Number of negative samples to generate. Default = 0\n",
    "    Returns\n",
    "    -------\n",
    "    word_pairs : list\n",
    "        Tuples (target_word, context_word, label). Label indicates if it is a positive sample (a true neighboor) or a neagtive one (a false one).)\n",
    "    '''\n",
    "    word_pairs = []\n",
    "    for sentence in corpus:\n",
    "        for i, target_word in enumerate(sentence):\n",
    "            context_words = []\n",
    "            for j in range(max(0, i - window_size), min(len(sentence), i + window_size + 1)):\n",
    "                if i != j:\n",
    "                    context_word = sentence[j]\n",
    "                    context_words.append(context_word)\n",
    "                    word_pairs.append([vocab[target_word], vocab[context_word], 1])\n",
    "\n",
    "            negative_samples = np.random.choice([w for w in corpus[window_size-1:-window_size] if w != target_word and w not in context_words][0], num_negative_samples)\n",
    "            for negative_samp in negative_samples:\n",
    "                word_pairs.append([vocab[target_word], vocab[negative_samp], 0])\n",
    "\n",
    "    return word_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Class\n",
    "\n",
    "\n",
    "This class constructs the Word2vec model. \n",
    "\n",
    "It takes two parameters:\n",
    "- vocab_size (int): the number of unique words (i.e., codes)\n",
    "- embedding_dim (int): the size of the embedding space\n",
    "\n",
    "\n",
    "The forward function takes two tensors as input:\n",
    "- target: the word on which Skip-Gram is applied\n",
    "- context: the context word\n",
    "\n",
    "\n",
    "It returns the score:\n",
    "\n",
    "$$v(c_{target})^Tv(c_{context})$$\n",
    "\n",
    "This is the matrix multiplication between the embedding (i.e., the representation) of the target word and that of its neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size : int\n",
    "        Number of unique words (ie. codes)\n",
    "    embedding_dim : int\n",
    "        Size of the embedding space.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim).to(device)\n",
    "    \n",
    "    def forward(self, target, context):\n",
    "        '''\n",
    "        Inputs\n",
    "        ------\n",
    "        target : tensor\n",
    "            The word on which one applies Word2Vec.\n",
    "        context : tensor\n",
    "            The context word.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dot_product : tensor\n",
    "            Score.\n",
    "        '''\n",
    "        target_embeds = self.embeddings(target)\n",
    "        context_embeds = self.embeddings(context)\n",
    "        dot_product = torch.sum(target_embeds * context_embeds, dim=1, keepdim=True) \n",
    "        return dot_product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is used to train the model.\n",
    "\n",
    "It takes 5 parameters:\n",
    "\n",
    "- embedding_dim (int): the dimension of the embedding\n",
    "- vocab (dict): unique words in the corpus\n",
    "- nb_epoch (int): the number of iterations\n",
    "- learning_rate (float): the gradient step size\n",
    "- batch_size (int): the size of batches\n",
    "\n",
    "\n",
    "Some notes regarding training:\n",
    "- Possibility of batching\n",
    "- Loss: Binary Cross-Entropy (BCE)\n",
    "- Optimizer: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_Word2Vec(data_loader, vocab, embedding_dim, nb_epoch, learning_rate, verbose=True) :\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_loader : Pytorch.Dataloader\n",
    "        (target, context, label)\n",
    "    vocab : dict\n",
    "        {'mot' : id}\n",
    "    embedding_size : int\n",
    "        Size of the embedding.\n",
    "    nb_epoch : int\n",
    "        The number of iterations.\n",
    "    learning_rate : float\n",
    "        The learning rate for the Stochatic Gradient Descent.\n",
    "    verbose : boolean\n",
    "        If true, the loss per epoch is printed. Default = True\n",
    "    '''\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = Word2Vec(len(vocab), embedding_dim)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(nb_epoch):\n",
    "        total_loss = 0\n",
    "        loss_batch = []\n",
    "        for target, context, label in data_loader:\n",
    "            target, context, label = target.to(device), context.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(target, context)\n",
    "            label_tensor = label.clone().detach().view(label.size()[0], 1).to(dtype=torch.float)\n",
    "            loss = criterion(output, label_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            loss_batch.append(loss)\n",
    "        \n",
    "        avg_loss = torch.stack(loss_batch).mean().item()\n",
    "\n",
    "        if verbose==True:\n",
    "            print('Epoch {}: Total Loss = {:.4f} - Average Loss : {:.4f}'.format(epoch, total_loss, avg_loss))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch of hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function optimizes the model hyperparameters.\n",
    "\n",
    "The hyperparameters tested are:\n",
    "\n",
    "- window_size: the size of the window (i.e., the number of considered neighbors)\n",
    "- emb_size: the size of the embedding\n",
    "- num_negative_samples: the number of generated negative samples\n",
    "- batch_size: the number of sequences per batch\n",
    "\n",
    "\n",
    "Here are the different steps of the function:\n",
    "\n",
    "1. Create the dataset based on\n",
    "    - window_size\n",
    "    - num_negative_samples\n",
    "\n",
    "2. Generate a train (80%) and test (20%) sample:\n",
    "    - Train: for model learning\n",
    "    - Test: for evaluating the loss on this sample\n",
    "\n",
    "3. Create dataloaders based on batch_size\n",
    "\n",
    "4. Train the model\n",
    "\n",
    "5. Compute the loss on the validation sample\n",
    "\n",
    "6. Stores, for each set of hyperparameters, the test loss in a dataframe\n",
    "\n",
    "7. Return a dataframe with the following columns:\n",
    "    - window_size\n",
    "    - num_negative_samples\n",
    "    - batch_size\n",
    "    - embedding_dim\n",
    "    - Total_Loss: the total loss on the validation sample\n",
    "    - Avg_Loss: the average loss obtained on the validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gridsearch_Train_Word2Vec(corpus, vocab, nb_epoch, learning_rate, window_size_list, num_negative_samples_list, batch_size_list, embedding_dim_list):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus : list\n",
    "        SÃ©quences de codes par patient : [[Code1, Code2, Code3], [Patient 2]]\n",
    "    vocab : dict\n",
    "        {'mot' : id}\n",
    "    nb_epoch : int\n",
    "        The number of iterations.\n",
    "    learning_rate : float\n",
    "        The learning rate for the Stochatic Gradient Descent.\n",
    "    window_size_list : list\n",
    "        List of the sizes of the window (ie. number of neighboors) to test.\n",
    "    num_negative_samples_list : list\n",
    "        List of the numbers of negative samples to generate to test. \n",
    "    batch_size_list : list\n",
    "        List of the number of samples per batch to test.\n",
    "    batch_size_list : list\n",
    "        List of the embeddings dimensions to test.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_hyperparameters : DataFrame \n",
    "        DataFrame with the total loss and average loss per batch on the validation set for all combinations of hyperparameters tested.\n",
    "    '''\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    df_hyperparameters = pd.DataFrame(columns=['window_size', 'num_negative_samples', 'batch_size', 'embedding_dim', 'Total_Loss', 'Avg_Loss'])\n",
    "\n",
    "    i=0\n",
    "    \n",
    "    for window_size in window_size_list :\n",
    "\n",
    "        for num_negative_samples in num_negative_samples_list :\n",
    "\n",
    "            # Data Preparation\n",
    "            word_pairs = Word2VecDataset(corpus, vocab, window_size, num_negative_samples=num_negative_samples)\n",
    "\n",
    "            # Train / Test Split\n",
    "            word_pairs_train, word_pairs_test= train_test_split(word_pairs, test_size=0.2, random_state=42)\n",
    "\n",
    "            # Dataloader constructions\n",
    "            for batch_size in batch_size_list :\n",
    "                dataloader_train, dataloader_test = DataLoader(word_pairs_train, batch_size=batch_size, shuffle=True), DataLoader(word_pairs_test, batch_size=batch_size, shuffle=True)\n",
    "                        \n",
    "                # Training of the model on the training set\n",
    "                for embedding_dim in embedding_dim_list :\n",
    "                    print('Window_size : {} - Num_neg_samples : {} - Batch_size : {} - Emb_size : {}'.format(window_size, num_negative_samples, batch_size, embedding_dim))\n",
    "                    model = Train_Word2Vec(dataloader_train, vocab, embedding_dim, nb_epoch, learning_rate, verbose=False)\n",
    "\n",
    "                    # Validation of the model\n",
    "                    total_loss = 0\n",
    "                    loss_batch_test = []\n",
    "                    for target, context, label in dataloader_test:\n",
    "                        target, context, label = target.to(device), context.to(device), label.to(device)\n",
    "                        output = model(target, context)\n",
    "                        label_tensor = label.clone().detach().view(label.size()[0], 1).to(dtype=torch.float)\n",
    "                        loss = nn.BCEWithLogitsLoss()(output, label_tensor)\n",
    "                        total_loss += loss.item()\n",
    "                        loss_batch_test.append(loss)\n",
    "\n",
    "                    avg_loss_test = torch.stack(loss_batch_test).mean().item()\n",
    "\n",
    "                    # Save the loss in the final dataframe\n",
    "                    df_hyperparameters.loc[i, 'window_size'], df_hyperparameters.loc[i, 'num_negative_samples'], df_hyperparameters.loc[i, 'batch_size'], df_hyperparameters.loc[i, 'embedding_dim'], df_hyperparameters.loc[i, 'Total_Loss'], df_hyperparameters.loc[i, 'Avg_Loss'] = window_size, num_negative_samples, batch_size, embedding_dim, total_loss, avg_loss_test\n",
    "                    \n",
    "                    i+=1\n",
    "\n",
    "    return df_hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Calcul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
