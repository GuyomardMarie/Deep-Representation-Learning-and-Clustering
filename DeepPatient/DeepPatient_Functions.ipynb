{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Patient - Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides functions to compute Deep Patient Representation Learning\n",
    "\n",
    "### Reference\n",
    "\n",
    "Miotto, R., Li, L., Kidd, B. A. et Dudley, J. T. (2016). Deep patient: an unsupervised representation to predict the future of patients from the electronic health records. Scientific reports, 6(1):1â€“10\n",
    "\n",
    "\n",
    "\n",
    "### Github with the authors codes\n",
    "\n",
    "https://github.com/riccardomiotto/deep_patient\n",
    "\n",
    "\n",
    "### Python environnement\n",
    "\n",
    "- Python 2.7\n",
    "- Required Packages : Theano, Scikit-learn, Pandas and Scipy\n",
    "\n",
    "\n",
    "### Complementary Functions\n",
    "\n",
    "We provide 2 complementary functionalities :\n",
    "\n",
    "1. A Gridsearch function: gridsearch_sda\n",
    "\n",
    "    This function aims at optimizing the following hyperparameters composing Deep Patient architecture:\n",
    "    - nhidden: dimension of the latent space\n",
    "    - nlayer: number of Autoencoder layers\n",
    "    - corrup_lvl: data corruption level\n",
    "\n",
    "    All set of hyperparameters to be tested given as input are used to train the model on a training set (80% of the sample). \n",
    "\n",
    "    The optimal set of hyperparameters is the one minimizing the loss function on a validation sample (20% of the sample).\n",
    "\n",
    "\n",
    "\n",
    "2. An Evaluation function : evaluate_sda\n",
    "\n",
    "    This function compute the cost of the model on both training and validation set.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for the Evaluation of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sda(model, train, test, verbose=True):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : SDA\n",
    "        The model to evaluate.\n",
    "    train : matrix\n",
    "        Training samples (matrix of size samples x features) use for the training.\n",
    "    test : matrix\n",
    "        The validation samples (size samples x features) on which to test the model.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    cost_per_layer_train : list\n",
    "        List of costs obtained for each layer (ie. DA) on the training sample.\n",
    "    cost_per_layer_test : list\n",
    "        List of costs obtained for each layer (ie. DA) on the validation sample.\n",
    "    '''\n",
    "\n",
    "    cost_per_layer_train = []\n",
    "    cost_per_layer_test = []\n",
    "\n",
    "    sda_train = SDA(train.shape[1],\n",
    "                nhidden=model.sda[0].nh,\n",
    "                nlayer=model.nlayer,\n",
    "                param={\n",
    "        'epochs': model.sda[0].epochs,\n",
    "        'learn_rate' : model.sda[0].learn_rate,\n",
    "        'batch_size': model.sda[0].batch_size,\n",
    "        'corrupt_lvl': model.sda[0].corrupt_lvl\n",
    "    })\n",
    "\n",
    "    sda_test = SDA(test.shape[1],\n",
    "                nhidden=model.sda[0].nh,\n",
    "                nlayer=model.nlayer,\n",
    "                param={\n",
    "        'epochs': model.sda[0].epochs,\n",
    "        'learn_rate' : model.sda[0].learn_rate,\n",
    "        'batch_size': model.sda[0].batch_size,\n",
    "        'corrupt_lvl': model.sda[0].corrupt_lvl\n",
    "    })\n",
    "\n",
    "    data_train = train\n",
    "    data_test = test\n",
    "\n",
    "    for i in range(model.nlayer) :\n",
    "            \n",
    "            sda_train.sda[i].w.set_value(model.sda[i].w.get_value())\n",
    "            sda_train.sda[i].b.set_value(model.sda[i].b.get_value())\n",
    "            sda_train.sda[i].bp.set_value(model.sda[i].bp.get_value())\n",
    "\n",
    "            sda_test.sda[i].w.set_value(model.sda[i].w.get_value())\n",
    "            sda_test.sda[i].b.set_value(model.sda[i].b.get_value())\n",
    "            sda_test.sda[i].bp.set_value(model.sda[i].bp.get_value())\n",
    "\n",
    "            try:\n",
    "                data_train = data_train.toarray()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                data_test = data_test.toarray()\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            data_train = sda_train.sda[i].normalizer.fit_transform(data_train)\n",
    "            dt_train = theano.shared(value=data_train.astype(theano.config.floatX), borrow=True)\n",
    "            x_train = T.matrix(name='dt_train')\n",
    "            tilde_x_train = sda_train.sda[i]._corrupted_input(x_train)\n",
    "            y_train = sda_train.sda[i]._hidden_representation(tilde_x_train)\n",
    "            z_train = sda_train.sda[i]._reconstructed_input(y_train)\n",
    "\n",
    "            data_test = sda_test.sda[i].normalizer.fit_transform(data_test)\n",
    "            dt_test = theano.shared(value=data_test.astype(theano.config.floatX), borrow=True)\n",
    "            x_test = T.matrix(name='dt_test')\n",
    "            tilde_x_test = sda_test.sda[i]._corrupted_input(x_test)\n",
    "            y_test = sda_test.sda[i]._hidden_representation(tilde_x_test)\n",
    "            z_test = sda_test.sda[i]._reconstructed_input(y_test)\n",
    "\n",
    "            gcst_train = - T.sum(x_train * T.log(z_train) + (1 - x_train) * T.log(1 - z_train), axis=1)\n",
    "            gcst_test = - T.sum(x_test * T.log(z_test) + (1 - x_test) * T.log(1 - z_test), axis=1)\n",
    "            loss_train = T.mean(gcst_train)\n",
    "            loss_test = T.mean(gcst_test)\n",
    "\n",
    "            compute_loss_train = theano.function(inputs=[x_train], outputs=loss_train)\n",
    "            compute_loss_test = theano.function(inputs=[x_test], outputs=loss_test)\n",
    "            loss_train_value = compute_loss_train(data_train)\n",
    "            loss_test_value = compute_loss_test(data_test)\n",
    "\n",
    "            if verbose==True:\n",
    "                print('Layer : ' +str(i))\n",
    "                print(\"Loss_train:\", round(loss_train_value,4))\n",
    "                print(\"Loss_test:\", round(loss_test_value,4))\n",
    "\n",
    "            cost_per_layer_train.append(loss_train_value)\n",
    "            cost_per_layer_test.append(loss_test_value)\n",
    "\n",
    "            if i < model.nlayer-1:\n",
    "                data_train = sda_train.sda[i].normalizer.transform(data_train)\n",
    "                dt_train = theano.shared(data_train, borrow=True)\n",
    "                data_train = sda_train.sda[i]._hidden_representation(dt_train).eval()\n",
    "\n",
    "                data_test = sda_test.sda[i].normalizer.transform(data_test)\n",
    "                dt_test = theano.shared(data_test, borrow=True)\n",
    "                data_test = sda_test.sda[i]._hidden_representation(dt_test).eval()\n",
    "                \n",
    "    return cost_per_layer_train, cost_per_layer_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridsearch of optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested Parameters:\n",
    "- nhidden: dimension of the latent space\n",
    "- nlayer: number of Autoencoder layers\n",
    "- corrup_lvl: data corruption level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_sda(data, epochs, learning_rate, batch_size, embedding_dim_list, layers_list, corrupt_lvl_list) :\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : matrix \n",
    "        Matrix : samples x features.\n",
    "    epochs : int\n",
    "        Number of epochs.\n",
    "    learning_rate : float\n",
    "        Learning rate of the Gradient Descent.\n",
    "    batch_size : int\n",
    "        Number of samples per batch.\n",
    "    embedding_dim_list : list\n",
    "        List of dimensions of the resulting embedding (ie. the hidden layer of the SDA) to test.\n",
    "    layers_list : list\n",
    "        List of the number of layers (ie. DA) to test.\n",
    "    corrupt_lvl_list : list\n",
    "        List of the values of data corruption (ie. noise) to test.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_gridsearch : DataFrame\n",
    "        For each pairs of parameters (nhidden, nlayer, corrupt_lvl) the associated loss on both training and test samples.\n",
    "    '''\n",
    "\n",
    "    seq_matrix_train, seq_matrix_test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "    i = 0\n",
    "    df_hyperparameters = pd.DataFrame(columns=['nlayer', 'nhidden', 'corrupt_lvl', 'Loss_train', 'Loss_test'])\n",
    "\n",
    "    for nlayer in layers_list :\n",
    "        for nhidden in embedding_dim_list :\n",
    "            for corrupt_lvl in corrupt_lvl_list :\n",
    "\n",
    "                sda = SDA(seq_matrix_train.shape[1],\n",
    "                                nhidden=nhidden,\n",
    "                                nlayer=nlayer,\n",
    "                                param={\n",
    "                        'epochs': epochs,\n",
    "                        'learn_rate' : learning_rate,\n",
    "                        'batch_size': batch_size,\n",
    "                        'corrupt_lvl': corrupt_lvl\n",
    "                    })\n",
    "                \n",
    "                sda.train(seq_matrix_train)\n",
    "                \n",
    "                cost_per_layer_train, cost_per_layer_test = evaluate_sda(sda, seq_matrix_train, seq_matrix_test, verbose=False)\n",
    "\n",
    "                df_hyperparameters.loc[i, 'nlayer'], df_hyperparameters.loc[i, 'nhidden'],  df_hyperparameters.loc[i, 'corrupt_lvl'], df_hyperparameters.loc[i, 'Loss_train'], df_hyperparameters.loc[i, 'Loss_test'] = nlayer, nhidden, corrupt_lvl, round(cost_per_layer_train[-1],4), round(cost_per_layer_test[-1],4)                  \n",
    "                i+=1\n",
    "\n",
    "    return df_hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python2.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
